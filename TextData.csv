TEXT,ID,REFERENCE,SOURCE
"Artificial Intelligence (AI) stands at the forefront of technological innovation, revolutionizing the way we perceive and interact with machines. In its essence, AI refers to the development of computer systems capable of performing tasks that traditionally required human intelligence. The field encompasses a spectrum of applications, from machine learning algorithms that enable computers to learn and improve from experience to advanced robotics mimicking human-like decision-making processes. As AI continues to evolve, its impact extends across various sectors, ranging from healthcare and finance to manufacturing and entertainment. The ethical considerations surrounding AI, such as privacy concerns and potential job displacement, underscore the importance of responsible development and deployment in this dynamic and transformative field.",1,1,1
"Artificial Intelligence has come a long way since its inception, marked by significant milestones like the development of expert systems, neural networks, and the emergence of deep learning. With advancements in processing power and data availability, AI systems are becoming increasingly sophisticated, tackling complex problems and contributing to advancements in natural language processing, computer vision, and autonomous systems. However, challenges persist, including ethical dilemmas, bias in algorithms, and the need for transparent decision-making processes. As we navigate the evolving landscape of AI, a balanced approach that harnesses the technology's potential while addressing its limitations is essential for shaping a future where AI benefits society at large.",1,1,1
"The future trajectory of Artificial Intelligence holds the promise of unprecedented advancements, from AI-driven medical diagnoses to autonomous vehicles transforming transportation. Striking a harmonious balance between innovation and ethical considerations will be pivotal in ensuring that AI serves as a force for positive change, enhancing human capabilities rather than replacing them. As researchers delve deeper into areas like explainable AI and ethical frameworks, the transformative potential of this technology continues to unfold, offering a glimpse into a future where AI augments human intelligence and contributes to solving some of the most pressing challenges facing humanity.",1,1,1
"In the interconnected landscape of the digital age, networking serves as the backbone of communication and information exchange. Networking refers to the intricate system of interconnected devices, protocols, and technologies that enable seamless data transfer between computers and other devices. From local area networks (LANs) facilitating communication within a confined space to wide area networks (WANs) connecting geographically dispersed locations, networking plays a pivotal role in shaping the way individuals, businesses, and entire societies access and share information. The evolution of networking has witnessed transformative shifts, from the advent of the internet to the emergence of cutting-edge technologies like 5G, which promise unprecedented speeds and connectivity.",1,1,1
"The foundational principles of networking revolve around the establishment of reliable and efficient communication channels. Protocols such as TCP/IP govern the rules for data transmission, ensuring data integrity and security. Networking technologies have evolved from traditional wired connections to wireless systems, enabling mobility and flexibility in how devices connect and communicate. With the rise of the Internet of Things (IoT), networking extends beyond traditional computing devices to include everyday objects embedded with sensors and connectivity, creating a vast ecosystem of interconnected devices.",1,1,1
"As we stand on the cusp of the next wave of networking innovations, technologies like edge computing and quantum networking hold the potential to redefine the way data is processed and transmitted. The ongoing pursuit of faster speeds, lower latency, and enhanced security underscores the dynamic nature of networking. From the smallest local networks to the global internet infrastructure, the world of networking continues to evolve, shaping the digital landscape and driving innovation in communication technologies.",1,1,1
"Chemistry, often referred to as the ""central science,"" is the study of matter, its properties, composition, structure, and the changes it undergoes during chemical reactions. At its core, chemistry provides a fundamental understanding of the building blocks of the universe, unraveling the mysteries of atoms and molecules. From the microscopic realm of subatomic particles to the vast complexity of organic compounds, chemistry encompasses a diverse range of disciplines, including organic, inorganic, physical, and analytical chemistry. The pursuit of knowledge in chemistry not only deepens our comprehension of the natural world but also fuels innovations with applications in medicine, materials science, and environmental sustainability.",1,1,1
"The exploration of the periodic table, a cornerstone of chemistry, reveals the organized structure of elements based on their atomic properties. The periodic table's elegance lies in its ability to predict the behavior of elements and guide the synthesis of new compounds. Through the lens of chemistry, researchers delve into the intricacies of chemical bonding, exploring how atoms come together to form molecules with distinct properties. Organic chemistry, focused on the study of carbon-containing compounds, plays a crucial role in understanding the complexity of life, from the molecular composition of DNA to the vast array of natural and synthetic compounds.",1,1,1
"The historical narrative of chemistry is woven with the threads of scientific breakthroughs and paradigm shifts. From the alchemical pursuits of ancient civilizations to the systematic investigations of Antoine Lavoisier, who laid the foundation for modern chemistry, the discipline has undergone transformative changes. The discovery of elements, advancements in spectroscopy, and the development of chemical theories, such as Dalton's atomic theory and Mendeleev's periodic law, mark key milestones in the historical tapestry of chemistry. Today, as we unravel the intricacies of nanotechnology and delve into the realm of quantum chemistry, the historical legacy of chemistry continues to shape our understanding of the material world.",1,1,1
"History, the collective memory of human civilization, serves as a timeless tapestry woven with the threads of events, individuals, and societies that have shaped our world. It is the study of the past, providing insights into the triumphs and tribulations of humanity, the evolution of cultures, and the interconnectedness of global civilizations. By examining historical narratives, we gain a deeper understanding of the forces that have shaped our present and illuminate potential trajectories for the future. As a multidisciplinary field, history encompasses political, social, economic, cultural, and intellectual dimensions, weaving a narrative that transcends geographical and temporal boundaries.",1,1,1
"The exploration of history begins with the examination of primary sources, artifacts, and historical records that offer windows into bygone eras. Historians employ critical analysis to interpret and reconstruct events, shedding light on the motivations, perspectives, and consequences that define historical moments. The study of history is not a static endeavor; it involves a constant reevaluation of narratives and perspectives, challenging preconceived notions and fostering a nuanced understanding of the complexities inherent in the human experience.",1,1,1
"Historical inquiry spans a vast chronological spectrum, from ancient civilizations and classical empires to the modern era marked by industrialization, globalization, and technological revolutions. The study of key historical periods, such as the Renaissance, the Age of Exploration, and the Industrial Revolution, provides crucial insights into the interconnectedness of human societies and the catalysts for societal transformation. The examination of historical events, revolutions, and conflicts enables us to discern patterns and learn from the triumphs and mistakes of the past.",1,1,1
"Astronomy, the oldest of the natural sciences, explores the vastness of the cosmos, encompassing the study of celestial objects, phenomena, and the underlying physical laws that govern the universe. From ancient civilizations gazing at the night sky to modern observatories peering into the depths of space, astronomy has been a relentless pursuit to unravel the mysteries of the cosmos. At its core, astronomy seeks to answer fundamental questions about the origins, evolution, and fate of the universe, inspiring wonder and curiosity about our place in the cosmos.",1,1,1
"The observational foundations of astronomy lie in the systematic study of celestial bodies, including stars, planets, galaxies, and cosmic phenomena like supernovae and black holes. Telescopes, both ground-based and space-borne, serve as windows to the universe, capturing light across the electromagnetic spectrum and revealing the hidden wonders of the cosmos. Through observations and measurements, astronomers chart the motions of celestial objects, unravel the dynamics of cosmic phenomena, and probe the depths of space and time, painting a vivid portrait of the universe's vastness and complexity.",1,1,1
"The historical narrative of astronomy is intertwined with humanity's quest to understand the heavens. From the astronomical observations of ancient civilizations, such as the Maya, Egyptians, and Greeks, to the groundbreaking discoveries of figures like Copernicus, Galileo, and Kepler, the history of astronomy is marked by paradigm shifts and scientific revolutions. Each milestone, from the heliocentric model of the solar system to the discovery of exoplanets orbiting distant stars, represents a triumph of human curiosity and ingenuity, reshaping our understanding of the cosmos and our place within it.",1,1,1
"Artificial intelligence (AI) is perhaps the oldest field of computer science and very broad, dealing with all aspects of mimicking cognitive functions for real-world problem solving and building systems that learn and think like people. Therefore, it is
often called machine intelligence (Poole, Mackworth, & Goebel, 1998) to contrast it to human intelligence (Russell & Norvig,
2010). The field revolved around the intersection of cognitive science and computer science (Tenenbaum, Kemp, Griffiths, &
Goodman, 2011). AI now raises enormous interest due to the practical successes in machine learning (ML). In AI there was
always a strong linkage to explainability, and an early example is the Advice Taker proposed by McCarthy in 1958 as a “program with common sense” (McCarthy, 1960). It was probably the first time proposing common sense reasoning abilities as
the key to AI. Recent research emphasizes more and more that AI systems should be able to build causal models of the world
that support explanation and understanding, rather than merely solving pattern recognition problems (Lake, Ullman, Tenenbaum, & Gershman, 2017).",0,0,0
"Artificial intelligence is a branch of computer science capable of analysing complex medical data. Their potential to exploit meaningful relationship with in a data set can be used in the diagnosis, treatment and predicting outcome in many clinical scenarios. Methods: Medline and internet searches were carried out using the keywords ‘artificial intelligence’ and ‘neural networks (computer)’. Further references were obtained by crossreferencing from key articles. An overview of different artificial intelligent techniques is presented in this paper along with the review of important clinical applications. Results: The proficiency of artificial intelligent techniques has been explored in almost every field of medicine. Artificial neural network was the most commonly used analytical tool whilst other artificial intelligent techniques such as fuzzy expert systems, evolutionary computation and hybrid intelligent systems have all been used in different clinical settings. Discussion: Artificial intelligence techniques have the potential to be applied in almost every field of medicine. There is need for further clinical trials which are appropriately designed before these emergent techniques find application in the real clinical setting.",0,1,0
"In the 21st century artificial intelligence (AI) has become an important area of research in virtually all fields: engineering, science, education, medicine, business, accounting, finance, marketing, economics, stock market and law, among others (Halal (2003), Masnikosa (1998), Metaxiotis et al. (2003), Raynor (2000), Stefanuk and Zhozhikashvili (2002), Tay and Ho (1992) and Wongpinunwatana et al. (2000)). The field of AI has grown enormously to the extent that tracking proliferation of studies becomes a difficult task (Ambite and Knoblock (2001), Balazinski et al. (2002), Cristani (1999) and Goyache (2003)). Apart from the application of AI to the fields mentioned above, studies have been segregated into many areas with each of these springing up as individual fields of knowledge (Eiter et al. (2003), Finkelstein et al. (2003), Grunwald and Halpern (2003), Guestrin et al. (2003), Lin (2003), Stone et al. (2003) and Wilkins et al. (2003)).",0,0,0
"In recent years, networking and collaboration have become increasingly popular in education. Local and national initiatives have stimulated a variety of cooperative arrangements, from groups of schools that have volunteered to work together, to groups that have been induced to do so in the context of incentives, to others that have been subject to direct external pressure to collaborate (West & Ainscow, 2006). These many initiatives have not typically been based on a clear understanding and definition of what is meant by networking in education, however. One of the few definitions found in education was coined by Hadfield, Jopling, Noden, O’Leary, and Stoll (2006) on behalf of the National College for School Leadership (NCSL) in England. A network is defined by them as: ‘‘groups or systems of interconnected people and organisations (including schools) whose aims and purposes include the improvement of learning and aspects of well-being known to affect learning’’ (Hadfield et al., 2006, p. 5). This, however, is both an overly broad definition of networking when one is discussing networking at the organisational level, and an overly prescriptive one. On the one hand, it appears to encompass networks of individuals, possibly even within schools, while, on the other hand, it is highly prescriptive in its insistence on one particular goal that would describe all network activity. As we will see below, fruitful forms of interschool networking do not always revolve around pupil learning.",0,0,0
"THE REDUCTION of energy consumption has become
a key issue for industries, because of economical, environmental and marketing reasons. If this concern has a
strong influence on electronics designers, the information and
communication technology sector, and more specifically the
networking field, is also concerned. For instance, data-centers
and networking infrastructure involve high-performance and
high-availability machines. They therefore rely on powerful
devices, which require energy-consuming air conditioning to
sustain their operation, and which are organized in a redundant
architecture. As these architectures are often designed to
endure peak load and degraded conditions, they are underutilized in normal operation, leaving a large room for energy
savings. In recent years, valuable efforts have indeed been
dedicated to reducing unnecessary energy expenditure, which
is usually nicknamed as a greening of the networking technologies and protocols.",0,1,0
"The systematic review from which the findings in this paper are presented is motivated
by a quest to establish the extent to which UK
companies are engaged in networking activities when seeking to develop their innovative
capacity. Specifically, the objectives of the review
are to:
(1) establish the nature of the relationship
between networking and innovation
(2) compare the degree and impact of networking behaviour in the UK with that of
businesses in competing countries
(3) explore examples and literature on the failure of business-to-business networks
(4) generate insights informing policies aimed
at fostering business-to-business networking
leading to greater innovative capacity and
(5) identify areas for future research for the
Economic and Social Research Council’s
(ESRC) research priorities board.
The Porter Report (Porter and Ketels 2003)
establishes that inter-organizational networking is critical for the development of innovative ability in firms. The extent to which UK
firms are involved in networking and how this
activity translates into innovative outcomes is,
however, less clear in the report. For instance,
Porter’s study concludes that the UK underperforms key competitors in this area but provides little in the way of evidence to justify
the claim.",0,0,0
"Over the last two decades, noble metal nanoparticles (NPs) have been the subject of extensive research in the frame of nanotechnology, mainly owing to their unique optical properties. Indeed, the free electron gas of such NPs features a resonant oscillation upon illumination in the visible part of the spectrum. The spectral properties of this resonance depend on the constitutive material, the geometry of the NP and its environment. This resonant electronic oscillation is called localized surface plasmon (LSP), and the field of research that studies the fundamentals and applications of LSP is known as nanoplasmonics.1 LSPs are accompanied by valuable physical effects such as optical near-field enhancement, heat generation and excitation of hot-electrons. Hence, plasmonic NPs can behave as efficient nanosources of heat, light or energetic electrons, remotely controllable by light. In nanoplasmonics, these properties have stimulated extensive basic research and already led to a wide range of applications in nanotechnologies.
Light and heat are physical quantities involved in many mechanisms in physics, chemistry and biology. Hence, a natural trend is to explore possible frameworks that could gain from the unique properties of metal NPs. So far, biology has been one of the areas of science that has benefited the most from nanoplasmonics. For instance, gold NPs as nanosources of heat are already at the basis of applications ranging from photothermal cancer therapy,2–4 bio-imaging,5 drug delivery6 and nanosurgery.7 Chemistry is another field of science that can potentially greatly profit from plasmonic NPs. Indeed, heat is a major parameter in any chemical reaction, light can be used to achieve high selectivity in chemical mechanisms thanks to quantum selection rules and adjustable photon energies, and electron transfer is the basis of redox reactions. Hence, the idea to use metal NPs as efficient nanosources of heat, light and electrons appears to be an appealing concept to both boost the yield of chemical reactions and improve their spatial and temporal control.

In this tutorial review, we survey recent advances in plasmon-assisted chemistry. The first section gives the readers the basics of nanoplasmonics. We successively describe the optical and thermal properties of metal NPs as well as the process of hot electron generation. In the second section we review the first class of studies in which the intense light field concentrated at the metal NP surface drives photochemical reactions. The third section focuses on exploiting the heat generated by metal NPs to control the yield of chemical reactions. Finally, the last section discusses how energetic electrons can be extracted from the NP to trigger chemical transformation of neighbouring reactants or enhance the efficiency of photocatalysts.",0,0,0
"Kohn–Sham DFT1–5 has in the recent past emerged as an important first-principles computational method6 – 15 to predict chemical properties accurately16 – 22and to analyze and interpret these in convenient and simple chemical terms.23 – 31 The reasons for its popularity and success are easy to understand. In the first place, the DFT approach is in principle exact. The exact exchangeand-correlation (XC) functional is unknown, but the currently available XC functionals provide in most cases already a “chemical” accuracy of a few kcal/mol for binding energies.16, 22, 32, 33 Moreover, the quest for more accurate ones based on a more detailed understanding of their essential properties is continuing.27, 28, 34 – 46 In the second place, it preserves at all levels of approximation the appealing one-electron molecular orbital (MO) view on chemical reactions and properties. The computed orbitals are suitable for the typical MO-theoretical analyses and interpretations.27, 29 – 31 The KS method effectively incorporates all correlation effects. In the third place, it is a relatively efficient computational method, and its fundamental scaling properties do not deteriorate when methodological precision is increased, in particular, when a more accurate XC functional is applied. Recent research paves the way to implementations that scale only linearly with the system size.15, 47 – 60 This brings within reach the treatment by fundamental quantum chemical methods of systems with hundreds, maybe even thousands of atoms.",0,0,0
"Cyclopropanol and its derivatives are carbocyclic homologues of enols, and there is an obvious similarity in chemical properties between these classes of compounds due to the well-known “unsaturated” character of the cyclopropane ring. Although cyclopropanols are usually less reactive than their olefinic “relatives”, their chemical properties are more diverse than those of enols or enolates. Cyclopropanols are able to undergo synthetically useful transformations with retention or cleavage of the strained three-carbon ring. In the latter case, cyclopropanols act formally as equivalents of homoenolates (C 1?C3 cyclopropane ring cleavage) or corresponding allylic derivatives (C 2?C 3 ring cleavage).
Cyclopropanol was first synthesized in 1942 by Cottle and co-workers.1,2 The development of cyclopropanol chemistry in the past decades has been quite fruitful, and these compounds occupied their own niche in synthetic practice as useful intermediates in organic synthesis 3-11 and as substances which are capable of possessing important kinds of biological activity. 12-16 The present review is written for the purpose of reflecting recent achievements in the areas of synthesis and synthetic applications of cyclopropanols. It may be regarded as a supplement to the earlier review by Gibson and De Puy.5",0,0,0
"Mesopotamia is known to scholars and laypeople alike as the cradle of civilization. Home to some of the earliest cities in the world, famous for the law codes of its kings and for the invention of writing, the rst home of the biblical Abraham, it has achieved a reputation as the birthplace of many of the hallmarks of Western civilization. Its name comes from a Greek word meaning the “land between the rivers” – the alluvial plains of the Tigris and the Euphrates, including large portions of the modern countries of Iraq and Syria ( g. 1.1). The ancient inhabitants of this region maintained contacts with people living beyond it, including those of the lowlands of southwestern Iran, the valleys of the Zagros Mountains, and the foothills of the Taurus Range. Rome was not built in a day, nor were the civilizations of ancient Mesopotamia. We will consider developments that occurred over the course of almost three millennia, from ca. 5000 to 2100 B.C. – in archaeological terminology the Ubaid, Uruk, Jemdet Nasr, Early Dynastic, and Akkadian periods (table 1.1). During these 3,000 years, Mesopotamia developed from a sparsely populated region in which the majority of settlements were small agricultural villages to a land of several hundred thousand people, most of them living in large cities and many engaged in specialized occupations. Architecture became increasingly grandiose and elaborate. An ever-wider array of raw materials was imported to manufacture utilitarian items and luxury goods and to adorn the temples of the gods and the homes of the rich and powerful. Despite this increasing material prosperity,the emergence of civilization was not a uniformly positive development. Along with the construction of impressive city walls and elaborate temples adorned with sculptures and inlaid with semiprecious stones and metals and the elaboration of artistic expression of all kinds came the exploitation of the “common” people. It was the labor of the majority that funded the trading expeditions, military conquests, and artisanal expertise responsible for the great works of art and architecture that we still admire today.",0,0,0
"In mid November 2012, Mongolia celebrated with great pomp the 850th birthday of Chinggis Khan (1162–1227). The celebrations included an international conference entitled “Chinggis Khan and Globalization.” While this was an impressive manifestation of the use of the Mongol Empire for constructing national identity, the conference’s title reflects the fascination of our globalized age with periods of early globalization such as Mongol rule in 13th and 14th century’s Eurasia. Indeed, the study of the Mongol Empire has made enormous strides in the past two decades, and its most notable impact is the shift of seeing the Empire not only in national or regional terms but from a holistic perspective, in its full Eurasian context. This focus, credited mostly to the seminal works of Thomas T. Allsen, also means that the scholarly literature now gives more space to topics that interest world historians such as the cultural, economic and religious exchanges that prevailed in Mongol Eurasia. Simultaneously, the Mongols’ image in the scholarly literature begins to shift from the barbarian warriors obsessed with massacres and plunder, to the Mongols as active promoters of cross-cultural connections, who even brought about the transition from the medieval to the modern world. Part of this new focus was enthusiastically embraced by world history textbooks, although these works often tend to relate to the Mongols more as a passive medium, the main contribution of which was the unification of a vast territory under one rule, and ignore the Mongols’ nomadic culture and its impact.2",0,0,0
"Without doubt, emperors in early modern Japan were impotent and the imperial court in Kyoto survived due to bakufu largesse. The Tokugawa military regime in Edo exercised de facto sovereign power. The shogun, not the emperor, took responsibility for Japan's defense and foreign re- lations; the shogun, not the emperor, conferred lands to daimyo and confiscated these from them. Politically conscious Japanese in early to mid-Tokugawa times believed that the emperor and court had proven their administrative incompetence by the time of Emperor Go-Daigo (r. 1318- 39). People assumed that only military governments could rule effectively in Japan after centuries of court corruption and decline that had culminated in the disastrous Jokyt War of 1221 and Kemmu Restoration of 1333-36. Tokugawa thinkers construed this fall of the imperial house leading to warrior and bakufu supremacy as ""historically irreversible."" "" As the Chu Hsi Confucian Muro Kyuso (1658-1734) noted, it ran contrary to reason in nature and human affairs to desire a never-ending imperial dynasty: ""No dynasty that has risen to power has ever avoided falling from it, [just as] no man given life has ever escaped death."" 2 The Sorai School thinkers, Dazai Shundai (1680-1747) and Yamagata Daini (1725-67), called Japan's im- perial house a ""defunct dynasty"" (shokoku).13 According to Kumazawa Banzan (1611-91), ""control of the realm will never revert to imperial court nobles; for even if we warriors restored it to them, [their rule] would not last for long."" 14 Or, as Yamaga Soko (1622-85) put it, ""even myriad oxen could not return the imperial court to the power it enjoyed in antiquity",0,0,0
"Europa, the second of the four Galilean satellites in its distance from Jupiter, is an ice-covered body with a radius of 1560 km, about the same size as Earth’s Moon (1738 km). Europa is locked in a tidal resonance with its neighboring satellites, volcanic Io on the inside and large Ganymede on the outside. Like each of the Galilean moons, Europa is spin-locked to Jupiter, rotating about its pole with a period identical to its orbital period of 3.6 Earth days. The resonance keeps the satellites from perfect circular orbits, inducing a forced eccentricity that varies their distances from Jupiter and results in tidal flexing and internal dissipation of energy. This heating is most intense at Io, due to its proximity to the giant planet Jupiter, and results in constant volcanic activity on that small moon. A substantial amount of heat is dissipated within Europa’s core, mantle, and ice shell as well (Cassen et al., 1979, 1980; Ojakangas and Stevenson, 1989). Gravity measurements made by the Galileo spacecraft (Anderson et al., 1998) show that Europa is an internally differentiated rocky body with about 100 km of material of density ?1000 kg m?3 at its surface, virtually certainly water in either solid or liquid phase. If this water were predominantly liquid, it would be more than double the volume of all of Earth’s oceans combined. Europa’s surface varies in temperature from about 50 K near the poles to as much as 120 K at the equator (Ojakangas and Stevenson, 1989; Spencer et al., 1999). Its tenuous atmosphere, due mostly to oxygen sputtered from its surface ice (Hall et al., 1995; Sieger et al., 1998), is in fact a near-vacuum. Liquid water exposed at Europa’s surface would rapidly boil and freeze; once freezing progressed to about half a meter the pressure of this ice cap would exceed the vapor pressure of the underlying water and boiling would cease (Reynolds et al., 1983).",0,0,0
"Juno crossed Jupiter’s polar regions on 27 August 2016 and performed the first plasma and wave measurements in the low-altitude auroral regions of Jupiter (see Bagenal et al. [2014], for review of Juno’s magnetospheric objectives). This constitutes the first opportunity to directly investigate in situ the properties of the Jovian auroral radio emissions and measure the features of the electron populations that may be responsible for their generation. Analysis at similar levels of detail was only possible at Earth with spinning spacecraft such as S3-3, Viking, and FAST [Omidi et al., 1984; Roux et al., 1993; Ergun et al., 1998]. The other unique example of extraterrestrial source crossing was at Saturn with Cassini [Lamy et al., 2010]. One objective of this exploration is to understand the scenario for generation of one of the most powerful nonthermal radio emissions in the solar system, well explored since the dawn of radio astronomy [Burke and Franklin, 1955] by professional and amateur observers. Such a scenario is likely of general astrophysical use [Melrose and Dulk, 1982]. It is now widely recognized that the cyclotron maser instability (CMI) is the generation mechanism of the nonthermal auroral radio emissions (see Wu [1985] for a review). A large number of theoretical studies and observations support this mechanism (see Zarka [1998] for a review). The CMI indeed appears to be particularly capable of the amplification of X-mode waves at frequencies close to the electron gyrofrequency in a tenuous plasma (? = (fp/fc) 2 ? 1 where fp and fc are respectively the electron plasma and gyrofrequency) provided that the electron distribution presents positive ?f/?v? gradients (v? is the velocity perpendicular to the static B field). These are very general features of the electron velocity distributions observed in the auroral regions which is why the CMI theory is widely successful.",0,0,0
"The favored model for the formation of the Moon is the “Giant Impact”: during the last stage of terrestrial planet formation, planetary embryos of Moon- to Mars-size (approximately 0.01–0.1 Earth masses, ME) collide in a sequence of massive mutual collisions – called Giant Impacts – until all or most of them have been accreted into a few, large rocky planets (e.g., Weidenschilling, 2000). In the Solar System, two “fully grown” rocky planets – Venus and Earth – formed, while two likely planetary embryos – Mercury and Mars – remained on stranded orbits (e.g., Hansen, 2009). A planetary embryo having between one fourth to two Mars masses (depending on the Giant Impact model; see below), called “Theia” after the mythological mother of the Greek Moon-goddess Selene, then collides with the almost fully formed, differentiated proto-Earth, a few 10 Ma after the formation of the Solar System. Such a Giant Impact can explain the observed physical properties of the Earth–Moon system: the high angular momentum, the iron-deficiency of the bulk Moon and the high mass ratio between the Moon and Earth (Hartmann and Davis, 1975, Cameron and Ward, 1976).",0,0,0
"Cryptography, the science of secure communication, plays a crucial role in safeguarding sensitive information in the digital age. At its core, cryptography involves the use of mathematical algorithms and techniques to encrypt data, ensuring that only authorized parties can access and decipher it. From ancient ciphers used by military commanders to modern encryption protocols securing online transactions, cryptography has evolved to meet the growing challenges of cybersecurity. The principles of cryptography encompass confidentiality, integrity, and authenticity, providing a framework for protecting data privacy and preventing unauthorized access (Stinson, 2005). As technology advances and cyber threats become increasingly sophisticated, cryptography continues to innovate, employing techniques such as quantum cryptography and homomorphic encryption to stay ahead of adversaries and preserve the confidentiality of information (Gollmann, 2010). In an era marked by data breaches and privacy concerns, cryptography serves as a critical tool in preserving trust and ensuring the security of digital communications.",1,0,0
"Cryptographic techniques have a rich history dating back thousands of years, with civilizations like ancient Egypt and Greece employing rudimentary methods of secret writing to convey sensitive messages (Singh, 2000). The advent of computers in the 20th century revolutionized the field of cryptography, enabling the development of complex algorithms and cryptographic protocols. One of the most significant breakthroughs came with the invention of public-key cryptography, which introduced the concept of asymmetric encryption, where different keys are used for encryption and decryption (Diffie & Hellman, 1976). This innovation paved the way for secure communication over insecure channels, laying the foundation for the modern internet and electronic commerce.",1,0,0
"The field of cryptography continues to evolve rapidly, driven by both advancements in technology and the increasing prevalence of digital communication in everyday life. With the rise of quantum computing posing new challenges to classical cryptographic systems, researchers are exploring post-quantum cryptography algorithms that can resist quantum attacks (NIST, 2021). Additionally, the emergence of blockchain technology has led to the development of novel cryptographic techniques for securing decentralized networks and enabling trustless transactions (Nakamoto, 2008). As cryptography adapts to meet the demands of an interconnected world, its significance in ensuring the confidentiality, integrity, and authenticity of information remains paramount (Schneier, 2015).",1,0,0
"Agriculture, the foundation of human civilization, encompasses the cultivation of crops, the raising of livestock, and the management of natural resources to sustainably produce food, fiber, and other essential commodities. From the dawn of agriculture in ancient Mesopotamia to the modern industrialized farming practices of the 21st century, agriculture has been central to human survival and societal development. The agricultural sector plays a critical role in feeding the world's growing population, addressing global hunger, and fostering economic growth and rural development. With the advent of technology and scientific advancements, agriculture has undergone profound transformations, from traditional subsistence farming to mechanized agriculture and precision farming techniques (Godfray et al., 2010).",1,0,0
"The history of agriculture is a narrative of innovation and adaptation, marked by the domestication of plants and animals, the development of irrigation systems, and the dissemination of agricultural knowledge across civilizations. Ancient agricultural practices, such as crop rotation and terracing, laid the groundwork for sustainable land management techniques still used today. The agricultural revolution of the 18th and 19th centuries saw the mechanization of farming processes, leading to increased productivity and the transition from rural agrarian societies to urbanized industrial economies (Smil, 2011). As agriculture continues to evolve, driven by technological innovations such as genetically modified crops and precision agriculture, the sector faces new challenges, including climate change, resource depletion, and food security (Foley et al., 2011).",1,0,0
"The future of agriculture lies in embracing sustainable practices that balance economic viability with environmental stewardship and social equity. From agroecology and organic farming to agrotechnology and biotechnology, researchers are exploring innovative approaches to enhance agricultural productivity while minimizing environmental impacts (Pretty et al., 2008). By harnessing the power of data analytics, remote sensing, and artificial intelligence, farmers can optimize resource use, improve crop yields, and mitigate the effects of climate variability (Ray et al., 2019). As the global population continues to rise, the challenge of feeding a growing world necessitates a holistic approach to agriculture that prioritizes resilience, sustainability, and inclusivity (Tilman et al., 2011).",1,0,1
"Biology, the study of life, encompasses a vast array of disciplines, ranging from molecular biology and genetics to ecology and evolutionary biology. At its core, biology seeks to understand the fundamental processes that govern living organisms, from the molecular mechanisms of cellular function to the complex interactions within ecosystems. By unraveling the mysteries of DNA, proteins, and genes, biologists gain insights into the genetic basis of life and the mechanisms driving evolution and diversity (Alberts et al., 2014). The interdisciplinary nature of biology fosters collaboration across scientific domains, enabling researchers to tackle complex questions spanning from the molecular mechanisms of cellular function to the global impacts of climate change (Purves et al., 2017).",1,0,0
"The exploration of biology begins with the study of cells, the basic units of life, and the molecular processes that govern their behavior. From the intricacies of cell signaling pathways to the regulation of gene expression, cellular biology provides a foundation for understanding the mechanisms underlying development, growth, and disease (Molecular Biology of the Cell, 2002). Evolutionary biology offers insights into the origins and diversification of life on Earth, tracing the evolutionary history of species and elucidating the processes driving genetic variation and adaptation (Futuyma & Kirkpatrick, 2017). Ecology explores the interactions between organisms and their environment, shedding light on the dynamics of ecosystems, population dynamics, and the interconnectedness of life forms (Krebs, 2009).",1,0,0
"As biology continues to advance, propelled by technological innovations and interdisciplinary collaborations, the field holds promise for addressing pressing challenges facing humanity, from combating infectious diseases to mitigating the impacts of climate change (Campbell et al., 2014). By integrating knowledge across scales, from the molecular mechanisms of disease to the global patterns of biodiversity, biologists strive to develop holistic approaches to preserving life on Earth and improving human well-being (Mayr, 2001).",1,0,0
"Archaeology, the study of human history through the excavation and analysis of material remains, offers insights into the cultural, social, and technological developments of past civilizations. From ancient artifacts and architectural structures to buried cities and human fossils, archaeology provides a window into the lives and societies of our ancestors. At its core, archaeology seeks to reconstruct the past, piecing together fragments of evidence to unravel the mysteries of human history and heritage (Renfrew & Bahn, 2018). By employing scientific methods and interdisciplinary approaches, archaeologists illuminate the complexities of ancient civilizations and their enduring legacies (Trigger, 2006).",1,0,0
"The practice of archaeology encompasses a diverse range of methodologies and techniques, from remote sensing and geophysical surveys to stratigraphic analysis and radiocarbon dating. Through meticulous excavation and documentation, archaeologists uncover artifacts and ecofacts buried beneath the earth's surface, reconstructing past landscapes and human activities (Renfrew & Bahn, 2018). The interdisciplinary nature of archaeology fosters collaboration across fields such as anthropology, geology, and conservation science, enabling researchers to contextualize archaeological findings within broader cultural and environmental frameworks (Scarre, 2013).",1,0,0
"As archaeology continues to evolve, technological advancements are revolutionizing the field, enabling new methods of exploration and analysis. Remote sensing technologies, such as LiDAR (Light Detection and Ranging), aerial drones, and satellite imagery, allow archaeologists to map landscapes and identify hidden archaeological features without disturbing the ground (Chase & Chase, 2014). Additionally, advances in DNA analysis and isotopic studies offer insights into ancient populations' genetic ancestry, migration patterns, and dietary habits (Hofman et al., 2020). These innovative approaches not only enhance our understanding of the past but also contribute to heritage conservation and cultural preservation efforts (Graham & Weingarden, 2020).",1,0,1
"Palaeontology, the study of prehistoric life through fossils and geological evidence, offers a window into the Earth's ancient past and the evolution of life forms over millions of years. From the discovery of dinosaur bones to the analysis of ancient ecosystems, palaeontology provides insights into the origins, diversity, and extinction events that have shaped the history of life on Earth (Benton, 2015). At its core, palaeontology integrates principles from biology, geology, and evolutionary science to reconstruct past environments, understand evolutionary processes, and document the rise and fall of ancient organisms (Brusatte, 2018).",1,0,0
"The exploration of palaeontology begins with the discovery and excavation of fossils preserved in sedimentary rocks, revealing the remains of long-extinct organisms and the ecosystems in which they lived. By studying fossilized bones, teeth, shells, and trace fossils, palaeontologists reconstruct ancient life forms, deciphering their anatomy, behavior, and evolutionary relationships (Carroll, 1988). The fossil record provides a unique window into the evolutionary history of life on Earth, documenting key transitions, such as the emergence of land-dwelling vertebrates, the rise of flowering plants, and the extinction of dinosaurs (Knoll et al., 2007).",1,0,0
"As palaeontology continues to advance, new techniques and methodologies are transforming our understanding of ancient life and the processes that have shaped the Earth's biosphere. High-resolution imaging technologies, such as CT scanning and synchrotron radiation, enable researchers to visualize internal structures of fossils and analyze minute details without damaging specimens (Briggs & Crowther, 2001). Isotopic analysis and geochemical studies provide insights into ancient climates, environments, and ecological interactions, shedding light on past ecosystems' dynamics and resilience (Falkowski et al., 2005). By integrating traditional fieldwork with cutting-edge scientific techniques, palaeontology remains at the forefront of scientific inquiry, unraveling the mysteries of life's history and informing our understanding of the planet's past, present, and future.",1,0,0
"Cryptography has a long and fascinating history. The most complete non-technical account of the subject is Kahn’s The Codebreakers. This book traces cryptography from its initial and limited use by the Egyptians some 4000 years ago, to the twentieth century where it played a crucial role in the outcome of both world wars. Completed in 1963, Kahn’s book covers those aspects of the history which were most significant (up to that time) to the development of the subject. The predominant practitioners of the art were those associated with the military, the diplomatic service and government in general. Cryptography was used as a tool to protect national secrets and strategies. The proliferation of computers and communications systems in the 1960s brought with it a demand from the private sector for means to protect information in digital form and to provide security services. Beginning with the work of Feistel at IBM in the early 1970s and culminating in 1977 with the adoption as a U.S. Federal Information Processing Standard for encrypting unclassified information, DES, the Data Encryption Standard, is the most well-known cryptographic mechanism in history. It remains the standard means for securing electronic commerce for many financial institutions around the world. The most striking development in the history of cryptographycame in 1976 when Diffie
and Hellman published New Directions in Cryptography. This paper introduced the revolutionary concept of public-key cryptography and also provided a new and ingenious method for key exchange, the security of which is based on the intractability of the discrete logarithm problem. Although the authors had no practical realization of a public-key encryption scheme at the time, the idea was clear and it generated extensive interest and activity
in the cryptographic community. In 1978 Rivest, Shamir, and Adleman discovered the first
practical public-key encryption and signature scheme, now referred to as RSA. The RSA
scheme is based on another hard mathematical problem, the intractability of factoring large
integers. This application of a hard mathematical problem to cryptography revitalized efforts to find more efficient methods to factor. The 1980s saw major advances in this area
but none which rendered the RSA system insecure. Another class of powerful and practical
public-key schemes was found by ElGamal in 1985. These are also based on the discrete
logarithm problem.",0,1,0
"Modern cryptography abandons the assumption that the Adversary has available infinite computing resources, and assumes instead that the adversary’s computation is resource bounded in some reasonable way. In particular, in these notes we will assume that the adversary is a probabilistic algorithm who runs in polynomial time. Similarly, the encryption and decryption algorithms designed are probabilistic and run in polynomial time. The running time of the encryption, decryption, and the adversary algorithms are all measured as a function of a security parameter k which is a parameter which is fixed at the time the cryptosystem is setup. Thus, when we say that the adversary algorithm runs in polynomial time, we mean time bounded by some polynomial function in k. Accordingly, in modern cryptography, we speak of the infeasibility of breaking the encryption system and computing information about exchanged messages where as historically one spoke of the impossibility of breaking the encryption system and finding information about exchanged messages. We note that the encryption systems which we will describe and claim “secure” with respect to the new adversary are not “secure” with respect to a computationally unbounded adversary in the way that the one-time pad system was secure against an unbounded adversary. But, on the other hand, it is no longer necessarily true that the size of the secret key that A and B meet and agree on before remote transmission must be as long as the total number of secret bits ever to be exchanged securely remotely. In fact, at the time of the initial meeting, A and B do not need to know in advance how many secret bits they intend to send in the future. We will show how to construct such encryption systems, for which the number of messages to be exchanged securely can be a polynomial in the length of the common secret key. How we construct them brings us to anther fundamental issue, namely that of cryptographic, or complexity, assumptions. As modern cryptography is based on a gap between efficient algorithms for encryption for the legitimate users versus the computational infeasibility of decryption for the adversary, it requires that one have available primitives with certain special kinds of computational hardness properties. Of these, perhaps the most basic is a one-way function. Informally, a function is one-way if it is easy to compute but hard to invert. Other primitives include pseudo-random number generators, and pseudorandom function families, which we will define and discuss later. From such primitives, it is possible to build secure encryption schemes. Thus, a central issue is where these primitives come from. Although one-way functions are widely believed to exist, and there are several conjectured candidate one-way functions which are widely used, we currently do not know how to mathematically prove that they actually exist. We shall thus design cryptographic schemes assuming we are given a one-way function. We will use the conjectured candidate one-way functions for our working examples, throughout our notes. We will be explicit about what exactly can and cannot be proved and is thus assumed, attempting to keep the latter to a bare minimum.",0,1,0
"Cryptography has been of great importance to the military and diplomatic
communities since antiquity but failed,
until recently, to attract much commercial attention. Recent commercial
interest, by contrast, has been almost
explosive due to the rapid computerization of information storage, transmission, and spying.
Telephone lines are vulnerable to wiretapping, and if carried by microwave radio, this
need not entail the physical tapping of any wires.
The act becomes passive and almost undetectable. It recently came to light that the Russians were using the antenna farms on the roofs
of their embassy and consulates to listen in on
domestic telephone conversations, and that they
had been successful in sorting out some conversations to Congressmen.
Human sorting could be used, but is too
expensive because only a small percentage of
the traffic is interesting. Instead, the Russians
automatically sorted the traffic on the basis of
the dialing tones which precede each conversation and specify the number being called. These
tones can be demodulated and a microprocessor
used to activate a tape recorder whenever an
“interesting” telephone number (one stored in
memory) is detected. The low cost of such a
device makes it possible to economically sort
thousands of conversations for even one interesting one.
The problem is compounded in remote computing because the entire “conversation” is in
computer readable form. An eavesdropper can
then cheaply sort messages not only on the basis
of the called number, but also on the content of
the message, and record all messages that contain one or more keywords. By including a name
or product on this list, an eavesdropper will
obtain all messages from, to, or about the “targeted” person or product. While each fact by
itself may not be considered sensitive, the compilation of so many facts will often be considered
highly confidential.",0,1,0
"Geographer David Harris has made a useful distinction between two types of ""paleotechnic"" or primitive agriculture. In his typology, Harris regards farming systems as ""man-modified ecosystems"" and analyzes them in terms of species diver- sity, productivity, and stability or homeostasis (Harris 26).  1. Seed-crop cultivation-including early wheat farming in the Near East or
 early maize agriculture in Mesoamerica and Peru-is a relatively simple ecosystem
 because it consists of very few species (often only one) growing in nearly pure stands
 composed of large numbers of individuals. It is highly productive (as the later
 discussion of wheat and maize will show) but also very unstable because of its low
 species diversity; it requires constant human attention.
 2. Vegeculture-such as the cultivation of manioc in Amazonia or yams and taro
 in southeast Asia-is often a very complex ecosystem, because many different
 cultivated species may be grown in a single field. Although sometimes less produc-
 tive than intensive seed-crop cultivation, it is far more ecologically stable because
 of its species diversity; it more closely approximates the complexity of natural
 vegetation.
 Because most early centers for seed-crop farming were in arid regions with good
 archeological preservation, a great deal is known about early farming there. More-
 over, morphological changes in the seeds following domestication can usually be
 detected. On the other hand, most early centers for vegeculture were humid tropical
 regions with poor archeological preservation; and since many of these crops are
 planted by cuttings, roots, or some other vegetative part which shows little or no
 morphological change after domestication, we know much less about early vegecul-
 ture. Archeologists like Donald Lathrap (40) have long bemoaned the fact that we
 know so little about the origins of root crops, and their importance is so frequently
 ignored.
 That is, it is ignored by archeologists working in arid regions; but archeologists
 working in the tropics, in spite of the absence of data, frequently present theories
 of root crop cultivation which are at present untestable. For example, Lowe (45) has
 argued for early manioc cultivation on the Pacific Coast of Chiapas and Guatemala,
 citing as evidence small obsidian chips which are said to resemble those used in
 manioc graters in South America. Yet a sample of more than 50 preserved plants
 from 1000 B.C. on the Guatemalan coast, which Coe and I recovered in 1962 (Coe
 & Flannery 9), contains no manioc; it is, like all arid Mexican highland samples of
 the same time period, mostly maize. Complicating the picture still further is the fact
 that botanist Earle Smith (MacNeish 46) identified a seed of wild Manihot from one
 of MacNeish's Tamaulipas caves. The species was one which has never been domes-
 ticated (Smith 67), but overeager archeologists continue to cite it as an example of
 cultivated manioc. The result is that many archeologists in Mesoamerica ignore root
 crops, while others argue strenuously for them with no supporting data. I will try
 not to ignore them, but I have little concrete data to offer.",0,0,0
"The agricultural sector continues to play a crucial role for development, especially in low-income countries where the sector is large both in terms of aggregate income and total labor force. Having been a key preoccupation of developing country governments, donors and the international community during the 1960s and 1970s, agriculture disappeared from the development agenda in the 1980s and 1990s, only to reappear in the first decade of the 21st century because of neglect and underinvestment (see Fig. 1). There is renewed interest in the problems of the sector—not to a small extent thanks to the World Development Report 2008, Agriculture for Development (World Bank, 2007) and Agriculture at a Crossroads (IAASTD, 2009), both of which came from global consultative processes of scientists, decision makers and donor agencies. Donor countries have pledged large sums for investment in agriculture—for example, the G8 countries promised $22 billion during their meeting in Aquila, Italy in 2009. These pledges were made in the aftermath of three simultaneous global crises—food crisis, climate crisis, and financial crisis—and their aftermath. Food prices have spiked twice in a period of 4 years: the United Nations Food and Agriculture Organization (FAO) food price index peaked in June 2008, then hit another record high in March 2011. Drought, fires, and monsoon floods have destroyed harvests in many countries from Russia to Pakistan. In poor countries, this has led to hunger, worsening food insecurity, and vulnerability to poverty.",0,0,0
"A longstanding question in economics is why some countries are so much richer than others. Today, for example, income per capita in the world's richest countries is roughly 35 times greater than it is in the world's poorest coun- tries. Recent work (e.g., Rachel Ngai, 1999; Robert E. Lucas, 2000) argues that the proxi- mate cause of this disparity is that today's poor countries began the process of industrialization much later and that this process is slow. In this paper we argue that a model of struc- tural transformation provides a useful theory of both why industrialization occurs at different dates and why it proceeds slowly. A key impli- cation of this model is that growth in agricul- tural productivity is central to development, a message that also appears prominently in the traditional development literature (see e.g., Peter Timmer, 1988).",0,0,0
"If the dominant molecular representation of biology is to be displaced by something deeper, something more comprehensive and inspiring, we need first to step back, define molecular biology, and place the molecular era into proper historical perspective.
Despite the fact that historians may well declare the 20th to be “the great century” in biology (24), it was in the 19th century that biology really came of age; consolidating itself, ridding itself of much of its ancient burden of mystical claptrap, and defining the great biological problems: Pasteur had banished spontaneous generation for good. He, along with Koch, Haeckel, Cohn, Beijerinck, and others, had shown the living world to comprise far more than plants and animals. Darwin had demystified evolution and recast it scientifically. The cell had emerged as the basic unit of biology. The gene had begun to take form (in the mind's eye). Embryology, given an experimental dimension, was to become an ever deepening and fascinating puzzle. Add to this biology's perennial concern with the nature and significance of biological form, and you had a science well worthy of the name (40). The proof of this, if such were needed, was that the more “mature” sciences, first chemistry and then physics, began to treat biology as worthy of interest in its own right—as a source of interesting problems, not just interesting products (41).
The great problems of 19th century biology were of two conceptually quite different types, and this difference would be greatly enhanced in the climate within which 19th and (especially) 20th century biology developed. On the one hand were the “encapsulatable” problems, those of the gene and the cell. Understanding here lay very much in the parts. On the other hand were the holistic problems, evolution and the genesis and nature of biological form (organization), where the parts don't give a real sense of the whole.",0,0,0
"Coronaviruses are a family of enveloped RNA viruses that are distributed widely among mammals and birds, causing principally respiratory or enteric diseases but in some cases neurologic illness or hepatitis (Lai and Holmes, 2001). Individual coronaviruses usually infect their hosts in a species?specific manner, and infections can be acute or persistent. Infections are transmitted mainly via respiratory and fecal?oral routes. The most distinctive feature of this viral family is genome size: coronaviruses have the largest genomes among all RNA viruses, including those RNA viruses with segmented genomes. This expansive coding capacity seems to both provide and necessitate a wealth of gene?expression strategies, most of which are incompletely understood.
Two prior reviews with the same title as this one have appeared in the Advances in Virus Research series (Lai 1997, Sturman 1983). The earlier of the two noted that the recognition of coronaviruses as a separate virus family occurred in the 1960s, in the wake of the discovery of several new human respiratory pathogens, certain of which, it was realized, appeared highly similar to the previously described avian infectious bronchitis virus (IBV) and mouse hepatitis virus (MHV) (Almeida and Tyrrell, 1967). These latter viruses had a characteristic morphology in negative?stained electron microscopy, marked by a “fringe” of surface structures described as “spikes” (Berry et al., 1964) or “club?like” projections (Becker et al., 1967). Such structures were less densely distributed and differently shaped than those of the myxoviruses. To some, the fringe resembled the solar corona, giving rise to the name that was ultimately assigned to the group (Almeida et al., 1968). Almost four decades later, recognition of the same characteristic virion morphology alerted the world to the emergence of another new human respiratory pathogen: the coronavirus responsible for the devastating outbreak of severe acute respiratory syndrome (SARS) in 2002–2003 (Ksiazek 2003, Peiris 2003). The sudden appearance of SARS has stimulated a burst of new research to understand the basic replication mechanisms of members of this family of viral agents, as a means toward their control and prophylaxis. Thus, the time is right to again assess the state of our collective knowledge about the molecular biology of coronaviruses.
Owing to limitations imposed by both space and the expertise of the author, “molecular biology” will be considered here in the more narrow sense, that is, the molecular details of the cellular replication of coronaviruses. No attempt will be made to address matters of pathogenesis, viral immunology, or epidemiology. For greater depth and differences of emphasis in particular areas, as well as for historical perspectives, the reader is referred to the two excellent predecessors of this review (Lai 1997, Sturman 1983) and also to volumes edited by Siddell 1995, Enjuanes 2005.",0,0,0
"EAV-induced disease may have been documented for the
first time around the end of the nineteenth century (Pottie,
1888; Clark, 1892). Three of the four currently known
arteriviruses (EAV, LDV and SHFV) were first isolated and
characterized 30–40 years ago (Doll et al., 1957; Riley et al.,
1960; Palmer et al., 1968; Tauraso et al., 1968). The exception
is the porcine arterivirus PRRSV, of which different strains
emerged (apparently independently) in the USA and Europe
about 10–15 years ago (Terpstra et al., 1991; Wensvoort et al.,
1991; Collins et al., 1992). The outcome of arterivirus infection
can range from an asymptomatic, persistent carrier state to
abortion or lethal haemorrhagic fever. For the replication of all
arteriviruses, macrophages appear to be the primary target cell
(Plagemann, 1996). The host range of the currently known
arteriviruses is restricted to horses and donkeys (EAV), pigs
(PRRSV), mice (LDV), and several genera of African and Asian
monkeys (SHFV). Recently, chickens and mallard ducks which
were exposed to PRRSV in drinking water were reported to
shed the virus in their faeces, suggesting that they are
susceptible to PRRSV infection (Zimmerman et al., 1997).
In nature, EAV and PRRSV are assumed to be transmitted
primarily via the respiratory route (McCollum et al., 1971;
Wensvoort et al., 1992; Meredith, 1993; Timoney &
McCollum, 1993). The severity of disease can vary greatly and
seems to be influenced by the strain of the virus and the
condition and age of the infected animal. EAV and PRRSV
infections can be subclinical, but can also lead to a variety of
symptoms, including respiratory distress, transient fever and
necrosis of small muscular arteries, from which the name of the
family prototype, EAV, was derived. Both viruses can cause
abortion in pregnant animals, a property which contributed
significantly to their initial identification (Doll et al., 1957;
Terpstra et al., 1991). ",0,0,0
"The term experiment appears in a number of archeological co Generally it is used in connection with either field or analytic metho both categories it most often means a trial; a test undertaken for the of evaluating a new method. The appendix title, ""Experimental Techn in Atkinson's Field Archaeology, for example, refers to new field method were not commonly in use in 1946, the date of publication, but which were
 undergoing trial. Willey's (1953:1) study of settlement patterns in the Viru
 Valley, Peru, termed an experiment, involves testing both a field and analytic
 method and Rouse's (1939: 7, 9) Prehistory in Haiti, also called an experiment,
 is a test of a proposed analytic method.
 Something analogous to the thought or imaginative experiment (Benjamin
 1936:257) is often used preparatory to field work. Thus Thompson's (1954)
 excavation of a Roman aqueduct is guided by entertaining and manipulating
 mental images of the course the aqueduct reasonably could have taken. Less
 commonly, archeologists perform comparative experiments (Cox 1958:4) in
 field methods. In ""Observations on the Efficiency of Shovel Archaeology,"" for ex-
 ample, Meighan (1950) tries two field methods with the end of evaluating their
 efficiency relative to each other. Experiments have also played a role in evalu-
 ating whether the shape of certain objects resulted from human or natural
 agencies. Experiments in this category are most often associated with the enig-
 matic eoliths of the Old World (for example, Barnes 1939), but similar experi-
 ments relevant to New World problems have also been performed (for example,
 Harner 1956).
 Another category of experiments entails operations in which matter is
 shaped, or matter is shaped and used, in a manner simulative of the past.
 These experiments, which I call imitative experiments, differ significantly from
 all the above. The aim of imitative experiments is testing beliefs about past
 cultural behavior. If archeology is taken to be the study of past cultural be-
 havior, the imitative experiment is the keystone of experimental archeology.
 The present study is concerned solely with the imitative experimen",0,0,0
"Thirty-three years ago an important conference was convened in Andover to assess anthropological knowledge in the Northeast (61). In 1976 another major summary of this geographically diverse area will appear as a volume in the new Handbook of North American Indians (113) series. The present review is limited to synthesizing the trends and results of investigations since the Andover conference opened new directions for research and to developing one of these, ecology, as a framework for future investigations. A geographical area ranging from Minnesota to Newfoundland and from On- tario to Tennessee affords many opportunities to anthropological archeologists. In 1941 large areas were archeological terra incognita, radiocarbon dating had not yet been invented, and interpretative approaches centered on taxonomy, com- parative trait lists, and migration-diffusion. Since then numerous professionally trained Canadian and American archeologists have surveyed the gaps, carbon-14 dates have provided a chronological framework, and interests are generally evolutionary. Almost every state in this area has been summarized (Table 1). The realization that this complex landscape has been the residence of numerous prehistoric cultures with similar adaptations begs for an ecological explanation to account for the similarities and differences found throughout this region. Currently the ecological basis of most research diverges into three areas. Settlement archeology, following Chang (15), is the most descriptive with analysis focusing on activity areas, inhabitations, settlement types, and environmental locations of sites. Environmental archeology developed from the natural area approach of Wissler (130) and Kroeber (68) and the archeological work of Clark (17) and Griffin (44).",0,0,0
"A recent paper by the present authors on method and theory can archeology (Phillips and Willey 1953) was originally plann parts. The first, on space-time or historical integration, was to  followed by a shorter part on historical-developmental theory, t jects being considered as related but operationally distinct branche logical method and theory. In the process of writing, the shorter came the longer, and at the same time became more empirical than th winding up as an actual developmental scheme for New World a Publication as part of the original paper was no longer in questi material is therefore presented here.
 Culture-historical integration, as described in our first paper, is co with cultural forms, the plotting of these forms in space and tim inferred functions in context. There is, we believe, common agre these are archeology's primary tasks. It is in realization of this that t ologist defines ""units of culture"" for study. Our arguments in th paper were addressed to the questions of how such units should be for how they may best be related one to another. This, we maintained tional to further theoretical formulations in the fields of culture and change. Various trial examinations and hypotheses might tak this point. The historical-developmental analysis presented here is  of such possibilities.
 Let us start by asking what method of organization can be used to  archeological data on an America-wide scale? All Americanists reco whereas the archeological reconstructions of localities, regions,  may be treated in unified systems, we are unable to extend these  any successful or impressive manner, beyond the bounds of the major area."" This, in itself, is a highly interesting aspect of New Wor history. The fact that the great American art styles rarely, if ever, p the limits of the area of their characterization is surely a key to a ce tural understanding. Similarly, the relative scarcity of specific ob items,"" that are native to one area hearth and transmitted to another to the areal isolationism of New World cultures. What, then, is th the historical evidence that links the various American areas?",0,0,0
"Georges Cuvier is rightly regarded as `a paleontologist, perhaps the ®rst to deserve the name' (Coleman
1964, p. 114). However, palaeontology existed before there were palaeontologists. Many earlier writers
described fossils, but the term `fossil' was used for any object dug out of the ground (Rudwick, 1985).
Konrad Gesner, for example, who, from 1551 on, compiled an encyclopaedia of all animals (Historia
Animalium) that ran to 4500 pages, produced, in 1565, a beautifully illustrated compendium of all `fossils'
that included stones and gems. Indeed, Rudwick (1985) dated the history of palaeontology from the 28th of
July, 1565, the day that Gesner ®nished his book. Gesner, who died several months later in Vienna of the
plague, compared fossils with extant organisms which they resembled (a fossil `crab' with Pagurus, a
living crab, but also compared objects such as Glossopterae (tongue-stones) with the teeth of living sharks,
®guring both the tongue stone and the shark to scale in the same plate. Colonna (1616) and subsequently
Steno (see Rudwick, 1985) showed that tongue stones were indeed teeth of large sharks.
Gesner entertained various theories of the nature of the fossil objects he described: the remains of
animals that once lived; ideal types of animals that never were alive; a trick (of God?) to disturb the natural
order; a test of one's faith in God. The origins of palaeontology as a science may be traced to the
con®rmation by Cuvier in the early part of the nineteenth century, that impressions seen in stones were
traces of organisms that lived in the past, species removed from the earth by extinction. Palaeontology
became the zoology and botany (and later the biology) of organisms of the past and a branch of
comparative morphology (Haber 1959; Rudwick 1985, 1997; Young 1992). Cuvier's ability to reconstruct
extinct animals from a few skeletal elements, to reconstruct the muscular system from muscle scars
preserved on bones, and to assign specimens to genus and species from a single bone, was legendary.
Following Cuvier, nineteenth century morphologists such as Richard Owen, Thomas Henry Huxley,
William Buckland and Edward Forbes Jr described fossils [especially dinosaurs, other `reptiles'
(archosaurs) and Archaeopteryx] as they would describe the anatomy of living animals. They were
especially concerned with transitions between major groups of animals, such as between reptiles and birds
(Owen 1863, 1870; Huxley, 1868, 1870; see Ospovat 1976, 1995). Owen was known as the `British
Cuvier', although this title, or the `English Cuvier', or the `next Cuvier', also was applied to the anatomist
Robert Grant and to the palaeontologist Gideon Mantell (Desmond 1979; Cadbury 2000).",0,0,0
"WITHIN biology, the status of species is most peculiar.
On one hand, species are widely viewed as a central element of the evolutionary process, ‘the currency of biology’ (Agapow et al. 2004) and ‘one of the few
fundamental units in biology’ (Barton 2001). Species are
the basis for studies in most fields of biology (Hausdorf
2011). The process by which species arise – speciation –
is ‘responsible for biological diversity, at least of sexual
organisms, so it is hardly a minor topic’ (Futuyma 2005,
p. 197). The Modern Evolutionary Synthesis itself was to
a significant degree focused on the nature and origin of
species (Mayr 1980; Cain 2000, 2009), and interest in
species and speciation continues to be intense today. As
Coyne and Orr (2004, p. 1) noted, ‘[o]ne of the most
striking developments in evolutionary biology during the
last 20 years has been a resurgence of interest in the origin of species’, as represented, for example, by an outpouring of recent books (Claridge et al. 1997; Howard
and Berlocher 1998; Wilson 1999; Schluter 2000; Hey
2001; Schilthuizen 2001; Stamos 2003, 2007; Coyne and
Orr 2004; Dieckmann et al. 2004a; Gavrilets 2004; Grant
and Grant 2008; Price 2008; Butlin et al. 2009; Wilkins
2009; Nosil 2012).
On the other hand, despite this putative importance,
nearly every recent general discussion of species and speciation includes admissions of seemingly fundamental
ignorance and confusion. For example, ‘the argument on
the appropriate way(s) to define a “species” is still unsettled…’ (Gavrilets 2003, p. 2197); ‘there is no consensus
on what exactly a species is’ (Barton 2001, p. 325); and
‘There are no rules for determining the level at which
varieties, subspecies, species, and genera ought to be discriminated’ (Benton 2010, p. 1479). The study of speciation has been called the ‘continuing search for the
unknown and unknowable’ (Paterson 1981). ‘The problem of speciation’, wrote Provine, ‘exemplifies…a
sequence in which the relative assurance of earlier explanations unravels in the light of greater knowledge, leaving
confusion and disunity’ (1991, p. 201). ",0,0,0
"Virtual palaeontology is the study of fossils through interactive digital
visualizations, or virtual fossils. This approach involves the use of cuttingedge imaging and computer technologies in order to gain new insights into
fossils, thereby enhancing our understanding of the history of life. While
virtual palaeontological techniques do exist for handling two-dimensional
data (e.g. the virtual lighting approach of Hammer et al. 2002), for most
palaeontologists the field is synonymous with the study of three-dimensionally
preserved material, and the term is used in this context throughout this
book. Note also that the manual construction of idealized virtual models of
taxa (e.g. Haug et al. 2012, Fig. 11), while very much a worthwhile undertaking, is not included in the concept of virtual palaeontology followed herein.
The majority of fossils are three-dimensional objects. While compression
of fossils onto a genuinely two-dimensional plane does of course occur
(Figure  1.1a), it is the exception, and in most preservational scenarios at least an element of the original three-dimensionality is retained (Figure 1.1b).
Three-dimensional preservation retains more morphological information
than true two-dimensional modes, but typically this information is
problematic to extract. Isolation methods, of which several exist, are one
solution. Fossils may simply ‘drop out’ or be naturally washed out of rocks;
wet-sieving of poorly consolidated sediments mimics this process.
Specimens may also be extracted chemically, for example, by dissolving the
matrix (e.g. Aldridge 1990). These approaches are effective where applicable, but are prone to losing associations between disarticulated or weakly
connected parts of fossils, and to damaging delicate structures. Specimens
can also be physically ‘prepared’ out using needles, drills or gas-jet powder
abrasive tools (e.g. Whybrow and Lindsay 1990); while usually preserving
associations, this approach may also damage delicate structures, scales
poorly to small specimens, and cannot always expose all of a specimen.
Finally, isolation of a fossil only provides access to its surface.",0,0,0
