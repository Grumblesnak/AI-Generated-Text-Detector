# Import necessary packages
# Unused imports and such will be commented out for now to reduce warnings until they are or are confirmed not needed
import numpy as np
# import tensorflow as tf
import tkinter as tk
import pandas as pd
import docx
# from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.metrics import accuracy_score
from tkinter import Label, Text, Button, messagebox
from tkinter import filedialog
from PyPDF2 import PdfReader

# from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense
from keras.callbacks import EarlyStopping


# Training data is the most important part of the program
# A program could mechanically work (going over text, producing outputs, etc.)
# Without proper training data ("Here is what Human and AI text looks like") then it will lack proper functionality
# For the program to be truly efficient then it will most likely require hundreds examples and more detailed identification IDs
# Simple GUI built using Tkinter
class TextDetectorGUI:
    def __init__(self, master, model, tokenizer, base_training_data):
        self.master = master
        master.title("AI Text Detector")  # This will be the title of the GUI window

        self.label = Label(master, text="Input Suspected Text: ")  # Label to indicate what the below entry box is for
        self.label.pack()

        self.text_entry = Text(master, height=5,
                               width=50)  # What the label above points to, where text is input for analysis
        self.text_entry.pack()

        self.upload_button = Button(master, text="Upload Document", command=self.upload_document)
        self.upload_button.pack(side=tk.LEFT)

        self.replace_training_data_button = Button(master, text="Replace Training Data", command=self.replace_training_data)
        self.replace_training_data_button.pack(side=tk.RIGHT)

        self.analyse_button = Button(master, text="Scan",
                                     command=self.analyse_text)  # Button will call the program to analyse whatever text is within the above input
        self.analyse_button.pack(side=tk.LEFT)  # Positions the button on the left of the GUI window

        self.exit_button = Button(master, text="Exit",
                                  command=self.exit_program)  # Button will exit the program, first calling a confirmation window beforehand just in case
        self.exit_button.pack(side=tk.RIGHT)  # Positions the button on the right of the GUI window

        self.result_label = Label(master, text="")  # This will display the result of the analysis
        self.result_label.pack()

        self.model = model
        self.tokenizer = tokenizer
        self.base_training_data = base_training_data
        self.highlight_colour = "yellow"

    def upload_document(self):
        file_path = filedialog.askopenfilename(filetypes=[("Text files", "*.txt;*.docx;*.pdf")])
        if file_path:
            with open(file_path, "rb") as file:
                if file_path.lower().endswith(".txt"):
                    content = file.read().decode("utf-8")
                elif file_path.lower().endswith(".docx"):
                    doc = docx.Document(file)
                    content = "\n".join([paragraph.text for paragraph in doc.paragraphs])
                elif file_path.lower().endswith(".pdf"):
                    pdf_reader = PdfReader(file)
                    content = ""
                    for page_num in range(len(pdf_reader.pages)):
                        page = pdf_reader.pages[page_num]
                        content += page.extract_text()
                else:
                    messagebox.showwarning("Invalid File", "Upload a valid text, Word, or PDF document...")
                    return

            self.text_entry.delete("1.0", tk.END)
            self.text_entry.insert(tk.END, content)

    def replace_training_data(self):
        response = messagebox.askyesno("Warning", "Changing base training data may cause errors or affect program outputs. Continue?")
        if not response:
            return

        file_path = filedialog.askopenfilename(filetypes=[("CSV file", "*.csv")])
        if file_path:
            try:
                new_training_data = load_training_data(file_path)
                self.model, self.tokenizer = train_model(new_training_data)
                self.base_training_data = new_training_data
                messagebox.showinfo("Success", "Dataset has been updated...")
            except Exception as e:
                messagebox.showerror("Error", f"Error loading or training with new dataset:\n{str(e)}")

    def analyse_text(self):  # Method will receive the text to be analysed then updates the above label with the result
        input_text = self.text_entry.get("1.0", tk.END).strip()
        input_sequence = self.tokenizer.texts_to_sequences([input_text])
        input_vector = pad_sequences(input_sequence)
        print(f"Shape of Input Vector: {input_vector.shape}")
        probability = self.model.predict(input_vector)[0][0] * 100
        probability = round(probability, 2)  # Round the probability output down to two decimal points
        result_text = f"AI-generation probability: {probability}%"
        self.result_label.config(text=result_text)

    def exit_program(self):  # Method will ask for confirmation before exiting the program
        if messagebox.askokcancel("Exit", "Are you sure?"):
            self.master.destroy()


def load_training_data(file_path):
    # Pandas is used to read in the CSV file and encoded, if not encoded then an error will occur
    df = pd.read_csv(file_path, encoding="latin-1")
    return df


def train_model(training_data):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(training_data["TEXT"])

    X_text = tokenizer.texts_to_sequences(training_data["TEXT"])
    X_text = pad_sequences(X_text)
    additional_features = training_data[["REPETITION", "REDUNDANT", "PERSONALITY", "ACCURACY", "FLOW", "DETAIL", "LANGUAGE"]].values

    X = np.hstack((X_text, additional_features))

    y = np.array(training_data["ID"])

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

    vocab_size = len(tokenizer.word_index) + 1

    trained_model = Sequential()
    trained_model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=X_train.shape[1]))
    trained_model.add(SimpleRNN(20, return_sequences=True))
    trained_model.add(SimpleRNN(20))
    trained_model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    trained_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
    trained_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[es])

    return trained_model, tokenizer


if __name__ == "__main__":
    # The new training method is deferred to a function with the name of the CSV file that will use pandas
    # The training data, when broken down and returned is sent to the model to be trained
    training_data = load_training_data("TextData.csv")
    model, tokenizer = train_model(training_data)

    # Ensures the script is actually being run correctly, if it is then the GUI will run properly in a Tk event loop
    root = tk.Tk()
    app = TextDetectorGUI(root, model, tokenizer, training_data)
    root.mainloop()
